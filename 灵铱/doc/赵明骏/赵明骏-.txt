周工作总结（按天描述）
第一周：2018.3.12-2018.3.16
	1、python爬虫的工作环境已经搭建完成，能够正常的运行程序
	2、初步完成一点资讯的新闻（要闻）的爬取，能够正常的存储到数据库中，并且字段都能够准确的存储。
	3、今日头条的json加载数据中算法的部分，已经完成，代码需要完善

第二周：2018.3.19-3.23
	3.19日：
		Fdfs在cenos、ubuntu中能够正常的进行文件的存储：
		具体实现的方法如下：（cenos）
		下载fdfs_client-py压缩文件

		解压缩，通过 python setup.py install进行安装
		注意：如果报错python3: can't open file 'install': [Errno 2] No such file or directory解决方法：cd 需要安装的文件夹目录下

		进行文件测试：
		  from fdfs_client.client import Fdfs_client
		  client = Fdfs_client('/etc/fdfs')
		然后：vi fdfs 
		tracker_server=自己ubuntu虚拟机的ip地址:22122
		ret = client.upload_by_filename('/home/texttext/xiaomao.jpg')
		>>> print ret
		输出结果：{'Status': 'Upload successed.', 'Storage IP': '172.18.115.34', 'Remote file_id': 'group1/M00/7F/CA/oYYBAFqvXW-AHsnTAAJDemKvPrI118.jpg', 'Group name': 'group1', 'Local file name': '/home/texttext/xiaomao.jpg', 'Uploaded size': '144.00KB'}

		Ubuntu的安装方式：
		Ubuntu的安装方式与cenos的安装一致，client = Fdfs_client('/etc/fdfs')改为client = Fdfs_client('/etc/fdfs/client.conf')

		未完成任务：
		在scrapy中下载图片到fdfs文件服务器，然后进行image_url与content中image_url的替换

	3.20日：
		1、在scrapy中下载文章的图片，并且保存到文件服务器的代码已经能够运行：
		实现的步骤为：
		1、保存文章的图片的image_url
		2、通过pipelines中的方法实现图片的下载
		3、在setting中设置文件储存的路径
		4、Save_fdfs脚本把路径中的文件存储到文件服务器
		5、然后替换文章内容中的src地址
		6、储存到mysql数据库

		2、分析中国网的法律内容，并完成代码，正常的运行程序

	3.21日：
		1、实现了中国网的全站爬取，使用scrapy框架,并成功存储到mysql数据库

		2、分析华律网的法律内容，并完成了代码的编写，正常的运行程序
		3、安装ubuntu系统
		(1)在deepin系统按F11进入boot menu
		(2)然后会自动启动光驱
		(3)具体操作步骤：
		https://www.cnblogs.com/fengliu-/p/7867673.html 部分内容可以参考

	3.22日：

		1、搭建python爬虫环境：
		(1)当报错
		E: 无法获得锁 /var/lib/dpkg/lock - open (11: 资源暂时不可用)
		E: 无法锁定管理目录(/var/lib/dpkg/)，是否有其他进程正占用它？
		解决：
		https://www.cnblogs.com/yidiandhappy/p/6396952.html

		2、spark安装，spark的地址为：172.18.113.180
		下载安装包，解压，在当前目录下开启命令行，输入：./Spark

		3、分析扬子晚报，完成了框架代码的编写，并能够正常的执行抓取任务
		4、分析北青网，完成了框架代码的编写，并能够正常的执行抓取任务

	3.23日：
	    
		1、完成扬子晚报的抓取任务
		2、完成北青网的抓取任务
		3、分析新浪网，并编写框架代码
		4、pymysql.err.InterfaceError数据库，最多300个连接。重启数据库，重写代码，解决问题
		
	
	周总结：通过一周的工作与学习，发现了遇到了之前并没有遇到的问题，比如文件服务器与python的交互，安装ubuntu系统（虽说没有难度），每天都会遇到不同的挑战
			这也是自我成长的阶段。
	
	
	
	第三周：2018.3.26-3.30
		
		3.26号：
			安装fdfs_client与python3的交互，总是在报错，无法进行正常的安装使用。
			pymysql.err.InterfaceError:解决华律网代码与mysql数据库交互异常的问题
			
			扬子晚报，代码已经初步完成
			
	   3.27号：
	        完成了fdfs_client与python3的交互，代码需要优化。希望部署之后不会报错
	
	
	    3.28号：
	        程序运行出错，主要是在替换整个文章的链接的问题上没有处理好。优化代码
	        华商报，代码已经初步完成
	
	
        3.29号:
            程序经过调试,修改代码已经能够正常的运行.后期需要实现mysql的异步存储,优化项目
            
        3.30号:
            代码优化,并把下载图片上传图片的模块单独出来
            程序部署成功
            
    3月份总结:
        
        入职已经有差不多一个月的时间,从中学到非常多的东西,部署环境,搭建fdfs文件服务器,等等.过程是非常心酸的,但是结果是令人满意的.非常庆幸能在这样的团队中成长.希望之后自己更加努力,积极配合,共同完成任务.
            
            
            
4.2-4.4号:
    4.2号:pymysql.err.InterfaceError: (0, '')pymysql存储错误,重构代码,采用存储redis中没有出错,再从redis中转存到mysql中
            
    4.3号:
        测试程序进行异步存储,能够成功的存储
        部署测试服务器,能够完成部署
        分析小视频网站,前期准备
            
    4.4号:
                 UnicodeEncodeError: 'ascii'
                 print('插入数据失败,原因：{},错误对象：{}'.format(failure,item))
                原因：
                   python print()函数输出编码问题
                解决方案：https://blog.csdn.net/butailengmu/article/details/78479505
                文件开头写入
                import io
                import sys
                sys.stdout = io.TextIOWrapper(sys.stdout.buffer,encoding='utf-8')
                
                分析56网,并完成代码的,程序能够正常运行,但是当运行速递过快硕鼠网站很容易被封
                
    4.8号:
    
        在运行美拍程序的时候报错,binascii.Error: Incorrect padding ,现在还没有找到可以很好解决的方法,只能先过滤掉这个错误
        
        分析人人视频,完成了人人视频的代码,测试运行没有问题,在运行3个小时之后,报错pymysql端口的错误
        
        分析美拍,完成美拍的代码,测试运行没有问题,主要就是binascii.Error: Incorrect padding错误.
        
        
    4.9号:
        
        测试56网站,早上刚开始的时候,硕鼠不会封,程序停止,下午再次运行的时候,发现已经封了,存储json70多条
        
        分析东方视频,完成了代码的编写,测试运行没有问题,已经把程序放在测试服务器上运行,看看长时间运行之后的结果
        
        修改了pipline中存储,改为连接池的方式
        
        
    4.10号:
        
        关于安装使用scrapy-deltafetch实现增量爬取:
            
            首先安装:安装Berkeley DB
            
            wget http://download.oracle.com/berkeley-db/db-5.3.28.tar.gz
            cd db-5.3.28/
            sudo dist/configure
            sudo make
            sudo make install
            然后:
            
            sudo pip3 install scrapy-deltafetch
            
            具体参考链接:http://jinbitou.net/2018/01/27/2581.html
        
        
        关于定时任务crontab的设置:每分钟运行一次的方法:
            
            首先定义hello.py文件
            编写test.sh内容:
                #!/bin/sh
                cd `dirname $0` || exit 1
                python  hello.py >> run.log 2>&1

            然后编写crontab -e
            
                * * * * * /usr/bin/python3 /home/action/Desktop/hellp.py >> /home/action/Desktop/run.log 2>&1
            
            执行./test.sh
            
            查看:cat run.log 或者 tail -f run.log
            
            
        实现了增量爬取的设置
        
        
    4.11号：
    
        通过unique insert ignore into添加唯一约束的方式，设置ALTER TABLE 表名 ADD UNIQUE KEY(‘需要约束的字段’);  实现title去重
        例如：ALTER TABLE Addrinfo ADD UNIQUE KEY (addrName)
        
        判断字段如果为空不添加的方法：
        
                    if item['NewsContent'] == ' ':
                        self.cur.execute('DELETE FROM tbl_NewsDetails WHERE NewsID=%s' % item["NewsID"])

        
        完善了东方头条视频的抓取，分不同类型字段的抓取。整体项目完成
            
            
   4.12：
    
         #关闭spider扩展
        #单位：秒 如果一个spider在指定的秒数后仍在运行，它将以 closespider_timeout 的原因被自动关闭，如果值设置为0（或者没有设置），spiders不会因为超时而关闭。
        
        CLOSESPIDER_TIMEOUT = 0
        #一个整数值，指定条目的个数。如果spider爬取条目数超过了指定的数， 并且这些条目通过item pipeline传递，spider将会以 closespider_itemcount 的原因被自动关闭。
        CLOSESPIDER_ITEMCOUNT = 0
        
        #一个整数值，指定最大的抓取响应(reponses)数。 如果spider抓取数超过指定的值，则会以 closespider_pagecount 的原因自动关闭。 如果设置为0（或者未设置），spiders不会因为抓取的响应数而关闭。
        
        CLOSESPIDER_PAGECOUNT = 0
        #一个整数值，指定spider可以接受的最大错误数。 如果spider生成多于该数目的错误，它将以 closespider_errorcount 的原因关闭。 如果设置为0（或者未设置），spiders不会因为发生错误过多而关闭。
        
        CLOSESPIDER_ERRORCOUNT = 0
                    
            
        切换到root账户 su root 如果失败可以sudo passwd 修改下密码，就可以切换
        切换到普通用户，su - action(用户)
        
        
        redis Desktop安装方法：
        
        1.  wget https://launchpadlibrarian.net/217261845/libicu52_52.1-8ubuntu0.2_amd64.deb
        2.  http://pan.baidu.com/s/1cA3jWU  #下载包
        
        3.  sudo dpkg -i libicu52_52.1-8ubuntu0.2_amd64.deb
        
            sudo apt-get -f install 
            sudo apt-get install zlib1g-dev
            sudo dpkg -i redis-desktop-manager_0.8.3-120_amd64.deb
            /usr/share/redis-desktop-manager/bin/rdm  （启动命令）
                        
        
        项目：美拍、人人视频、梨视频、东方视频，
                美拍——有两个报错还没有解决，但是不影响数据的储存，主要是个别视频在base64解码时候报错
                人人视频：分类没有做好
                梨视频：测试中，分类已经做好
                东方视频：测试中，虽然已经发送多个链接的请求，但是并没有执行，正在查找原因
        
        
    4.16-4.20:
    
        4.16号：
                完善北青网，华商报代码，程序运行正常，数据正常
                
                解决美拍的bug，主要是过滤了错误信息
                
                一点资讯，由于网站增加了反爬，加了cookie验证，之前的代码已经不能正常运行，已经通过cookie内部的算法，解析处数据
                
        4.17号：
        `   
            部署生产服务器，北青网，华商报，修改之后的美拍。可以在下载视频或者图片的请求出，加上retrying
            
            完善一点资讯，（图片的链接部分没有完善）
            
            分析58租房，发送了几十条链接信息，并没有发现异常。可以正常的获取部分数据，字段待完善
                
        4.18号：
        
            58同城租房，已经匹配好字段，并且能存储数据库，图片文件也能存储数据库，个别字段需要完善
            
            空闲时间 完善一点资讯代码。反爬比较厉害，需要进一步完善
            
        4.19号：
            主要在弄58同城对用的mysql字段中的内容，内容已经基本弄好
            
        4.20号：
        
            sudo yum install git在centos中安装git，主要是为了安装tesseract
            
            完成58求租的框架的搭建，并且成功入库，通过PIL图像切割的方法，获取电话信息，电话为图片加载
        
            
    4.16-20:周报：
        
        本周工作内容是：完成了三个新闻网站的项目内容的抓取，同时并成功存储到mysql、redis数据库中，解决了程序在运行过程中遇到的反爬措施。
        
                        分析58同城房屋栏目，主要是对58同城的租房，整租，合租，短租，求租，商铺的出租，写字楼的出租，新房，二手房部分的分析，已经对应字段的抓取。
                        现在已经完成了，对整租，合租，求租内容框架的搭建，已经内容的抓取。现在部分字段有待完善。
                        
    4.23-4.28号：
    
        4.23号：
        
            主要是对求租，整租，合租部分的完善，分析二手房部分，并逐步完善代码。
            
        
        4.24号：
            selenium已经弃用phantomjs，需要使用Selenium+Headless Chrome 无头的chrom浏览器https://blog.csdn.net/u010358168/article/details/79749149
            
            通过调用百度地图开发者API接口获取小区的坐标
            
            初步完成了二手房项目的抓取
            
        4.25号：
        
            完善了二手房，租房的字段
            
            10.100.102.150
            
            mSbW2WS0l0zhVDcZvP7uK2Ih03i5RHUg102-150

    
        4.26号：
        
           通过寻找unicode编码之后的信找出了58二手房中的地铁信息，开始通过chrome请求js获取信息也可以获取的，但是生产环境无法安装chromedriver
           
           部署生产环境完成。
           
           修改了二手房字段
           
           
            
    5.2-5.4：
    
        5.2号：
        
            完成了对写字楼出租，商铺出租，新房的测试，已经能够正常的访问数据
            
        5.3号：
        
            对新房，写字楼，商铺，短租进行测试，测试可以正常显示数据。但是下载时间必须设置较长时间，在不使用代理的情况
            
            查找，找男女朋友，找导师，找伙伴的网站；发现百姓网的网站比较符合，分析了百姓网网站（http://beijing.baixing.com/）。准备抓取数据
        
            
        周总结：
            
            完成了租房项目的抓取，后期需要优化项目。
            
            完成了找男朋友、女朋友、找导师、找伙伴，可以正在的在测试机上浏览数据
            
            【elasticsearch】访问地址http://172.18.113.113:9200/
            
            【logstash】172.18.113.123
            
            【kibana】172.18.113.128 

    5.7-5.11号：
    
        5.7号：
            
            优化了租房项目，现在主要抓取的是一些重点城市的房源信息
            
            优化了找朋友项目，等待部署到生产环境
            
            找酒店的相关项目，分析了携程酒店，准备获取相关的数据  SELECT * FROM hotel_management.hotel;
        5.8号：
            
            把酒店项目中的，携程酒店框架已经初步完成。后期需要进一步完善字段内容，可以正常的获取数据
            
            浏览有关于ELK日志系统的相关文章
            
        5.9号：
        
            招商管理项目， 
            
            SELECT * FROM merchants.project_finance;
            
            SELECT * FROM merchants.assets_transaction;
            
            SELECT * FROM merchants.attract_invest;
            
            找了几个相关招商管理的网站，但是感觉总是有些字段不能匹配，或者是没有，现在很是纠结
            
            ELk日志系统环境已经搭建完成，后期总结下搭建的具体步骤。
            
            找了一些文章以及课程有关于els日志系统的实践的，需要进一步学习；学习之后继续完善日志系统
            
        
            
        5.10号：
        
            把招商引资的框架已经写好，个别的字段需要后期的完善，抓取的网站是（选哪网https://www.xuannaer.com/yuanqu）
            
            找到了资产交易的网站（e交易：http://www.e-jy.com.cn/index）正在分析该网站
            
            招标信息的网站也可以在（e交易：http://www.e-jy.com.cn/index）抓取
            
            正在总结关于ELK日志系统的学习
            
        5.11号：
        
            项目融资、资金交易的框架已经写好，个别映射的字段，已经需要后期沟通的部分少数字段待完善，抓取的网站是（融诺网http://www.rongnuo.net）
            
            分析招标信息网站（e交易网站http://www.e-jy.com.cn/index）但是网站是信息不够全，招标网站大部分都是付费的，抓取比较有难度
            
            正在总结关于ELK学习
            
        
    本周优化了租房项目，现在主要抓取的是一些重点城市的房源信息，找朋友项目信息能够正常的获取数据，等待部署到生产环境；完成了招商管理项目中的项目融资、资产交易、招商引资，部分字段有待沟通完善；
    
    继续总结学习es相关的资料
        
        
                        
    5.14-5.18号：
        
        5.14号：
            
            总结有关于ELK日志系统相关的文章，以及内容
            
        5.15号：
        
            测通本地scrapy产生的日志可以存进elasticsearch并且在kibana中展示出来
            
            查找软件开发的网站，正在分析伯乐在线，准备抓取伯乐在线
            
    
        
        5.16号：
            
            完成了伯乐在线的字段，测试十几条数据没有发现问题。
            
            完善了梨视频（国内、国际分类），美拍（音乐、搞笑）分类
            
            
        5.17号：
            
            任务：
                找下探索、养生、故事相关的小视频网站
                
                把mysql中产生的日志存到es中，并展示
                
                用Tornado实现接口
            
            
            
            完成了小视频中国内、音乐、探索、养生、故事、国际、搞笑；待完善军事（需要重新找网站抓取）
            
            找了些文章关于mysql中产生的日志存到es中，实际操作没有成功，继续完善
            
    周总结：
        完成了学习板块中软件开发学习板块中的一个网站，但是网站中个别的开发语言没有涉及，需要再其他的网站；完善了小视频网站中的国内、音乐、探索、养生、故事、国际、搞笑、军事。其中美拍网站在测试中有一些
        问题出现，需要解决；把ELK日志系统初步的完善，能够把爬虫的日志输出到elasticsearch，并且能够展示。
        
    5.21-5.25号：
        本周继续完善ELk日志系统中kibana展示具体分析日志；总结学习tornado相关的内容；继续完善学习板块中的软件开发板块
        
        5.21号：
            
            可以把系统日志、爬虫产生的日志、mysql慢查询日志。写入到es中并可以展示。mysql错误日志没有成功的写入到es中。需要利用kibana进行日志的分析
            
            爬取了菜鸟教程中的教学内容，但是内容部分需要更好的完善
            
            
        5.22号：
            
            完成了软件开发文档部分的抓取，视频部分需要找网站，大的网站反爬比较厉害。
            
            完善了elk日志，能够把mysql慢查询的日志，mysql 错误日志存到es中，并可以展示。kibana展示不够直观，需要进一步完善
            
            查找tornado资料中。。。。
            
        5.23号：
            
            把新闻类的爬虫完善了，可以存到es中，把所有的新闻爬虫统一放到一起了，方便于管理。
            
            交规学习板块已经完善了
        
        5.24号：
            
            主要是学习tornado，与mysql链接，查询mysql数据库；与es链接，查询es数据库，但是查询不到数据，总是报错，正在分析原因
            
            学习板块中的育儿板块，分析网站久久母婴。已经写好了婴儿，育儿，学前的内容
            
        
        5.25号：
            
            把新浪新闻，一点资讯的框架已经写好了，需要完善的是字段，类型
            
            tornado测试中
            
        本周完成了ELK日志系统中关于系统日志，爬虫日志，mysq慢查询，mysql错误日志的获取展示。但是kibana中整体的视觉展示不够清晰，需要完善；
        完成了软件开发学习中的类型，完成了交规中的类型。写了部分的育学习板块。写了部分的新浪新闻，一点咨询。
        继续学习测试tornado与mysql的链接，与es的链接
    
    5.28-6.1号：
        
        5.28号：
            
            主要在学习tornado相关的知识，以及测试web api 接口，tornado与mysql已经测试通了，tornado与elasticsearch还需要测试
            
        5.29号：
            
            完成了一点资讯分类抓取，完成新浪新闻国际、国内新闻分类
            
            完成了tornado调用es数据的web 接口
            
            育儿学习板块中的分类信息写了有10个分类


        5.30号：
            
            修改之前的咨询类爬虫文件
            
            使用you-get解析视频，并下载，还没有实现批量下载
            
            分析今日头条
            
            查找育儿板块的网站





























